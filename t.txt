#pragma once
#include <stdio.h>
#include "thunderkittens.hpp"

using namespace kittens;

// Customize these to your liking
constexpr int CONSUMER_WARPGROUPS = 3;  // # HPC warpgroups that do the final LSE accumulation
constexpr int PRODUCER_WARPGROUPS = 1;  // # HPC warpgroups that do TMA loads
constexpr int NUM_WARPGROUPS      = (CONSUMER_WARPGROUPS + PRODUCER_WARPGROUPS);
constexpr int NUM_WORKERS         = (NUM_WARPGROUPS * WARPGROUP_WARPS);

// We'll double-buffer partial LSE in shared memory
constexpr int STAGES = 2;

//-----------------------------------------------------
// 2.1. Define a struct to hold the “globals” for TMA descriptors
//-----------------------------------------------------
template<int ROWS_>
struct final_lse_reduce_globals {
    // partial_lse shape: [num_chunks, ROWS_]
    // final_lse shape:   [ROWS_]
    using partial_tile = st_fl<ROWS_, 1>;  // We'll store chunk of partial LSE as 2D (ROWS_ x 1)
    using final_tile   = st_fl<ROWS_, 1>;  // We'll store final LSE as 2D (ROWS_ x 1)

    using partial_lse_gl = gl<float, -1, -1, -1, -1, partial_tile>;
    using final_lse_gl   = gl<float, -1, -1, -1, -1, final_tile>;

    partial_lse_gl partial_lse;  // global descriptor for partial LSE
    final_lse_gl   final_lse;    // global descriptor for final LSE

    int num_chunks; // how many partial-lse chunks we want to combine
    int ROWS;       // must match ROWS_, but can be used for bound-checks
};

//-----------------------------------------------------
// 2.2. The kernel
//-----------------------------------------------------
template<int ROWS>
__global__ __launch_bounds__(NUM_WORKERS * WARP_THREADS, 1)
void final_lse_reduce_ker(const __grid_constant__ final_lse_reduce_globals<ROWS> g)
{
    // HPC identification
    int warpid       = kittens::warpid();                // 0..(NUM_WORKERS-1)
    int warpgroupid  = warpid / WARPGROUP_WARPS;         // 0..(NUM_WARPGROUPS-1)
    int localwarp    = warpid % WARPGROUP_WARPS;         // which warp inside that HPC warpgroup

    // We'll allocate the double-buffer partial-lse in shared memory
    extern __shared__ int __shm[]; 
    tma_swizzle_allocator al((int*)&__shm[0]);

    // Type we store in SMEM for partial-lse chunk
    using partial_tile = st_fl<ROWS, 1>;

    // We'll have an array of [STAGES] partial tiles in shared memory
    partial_tile (&partial_smem)[STAGES] = al.allocate<partial_tile, STAGES>();

    //-----------------------------------------------------
    // HPC semaphores
    //-----------------------------------------------------
    // We'll need:
    //   partial_smem_arrived[stage]  => indicates partial-lse chunk is loaded
    //   compute_done[stage]          => indicates consumers are done with that chunk
    // So the producer doesn't overwrite it.
    __shared__ kittens::semaphore partial_smem_arrived[STAGES], compute_done[STAGES];
    if (threadIdx.x == 0) {
        for(int s=0; s<STAGES; s++){
            init_semaphore(partial_smem_arrived[s], 0, 1); 
            init_semaphore(compute_done[s], CONSUMER_WARPGROUPS, 0);
        }
    }
    __syncthreads();

    //-----------------------------------------------------
    // *** Producer warpgroup *** => last HPC warpgroup
    //-----------------------------------------------------
    if (warpgroupid == NUM_WARPGROUPS - 1) {
        // Usually we reduce register usage, as we just do memory ops
        warpgroup::decrease_registers<64>();

        // We'll do a pipeline over each chunk c = 0..(g.num_chunks-1)
        // We store it in partial_smem[c % STAGES].
        for (int c = 0; c < g.num_chunks; c++){
            int stage = c % STAGES;

            // Wait for the consumer to be done with this stage (unless c < STAGES)
            if (c >= STAGES) {
                wait(compute_done[stage], /*index=*/ (c / STAGES) % 2);
            }

            // Now we can load chunk c into partial_smem[stage]
            if (warpid == (NUM_WORKERS - 4)) {
                // We'll pick 1 warp (or a subset) to do the TMA call (example).
                // The coordinate must match partial_lse shape
                coord<partial_tile> tile_coord = { c, 0 }; 
                // Fill out-of-range with -∞ if c >= g.num_chunks, or rows > g.ROWS
                float neg_inf = base_types::constants<float>::neg_infty();

                tma::expect_bytes(partial_smem_arrived[stage], sizeof(partial_tile));
                tma::load_async(partial_smem[stage], g.partial_lse, tile_coord, partial_smem_arrived[stage], neg_inf);
            }
        }
        // After finishing all loads, we do no more. The consumer will eventually read the last chunk(s).

    //-----------------------------------------------------
    // *** Consumer warpgroup *** => gather partial-lse
    //-----------------------------------------------------
    } else {
        // Possibly we increase registers for HPC compute
        warpgroup::increase_registers<128>();

        // We'll hold the final row-wise LSE in registers:
        // We'll do the stable running approach:
        //   maxval[r], sumExp[r]
        // so final_lse[r] = log( sumExp[r] ) + maxval[r]
        // If sumExp[r] == 0 => final_lse[r] = -∞ (or handle corner cases).
        // For HPC demonstration, let's store them in arrays dimension [some small #].
        float maxval[ROWS], sumExp[ROWS];
        for (int r = 0; r < ROWS; r++){
            maxval[r] = base_types::constants<float>::neg_infty(); // -∞
            sumExp[r] = 0.f;
        }

        // We'll process each chunk c in turn
        for (int c = 0; c < g.num_chunks; c++){
            int stage = c % STAGES;

            // Wait for partial-lse chunk c to arrive in partial_smem[stage]
            wait(partial_smem_arrived[stage], (c / STAGES) % 2);

            // Now partial_smem[stage](row,0) is the partial LSE for chunk c
            // We do running stable sum:
            //   let x = partial_smem[stage](row,0)
            //   oldMax = maxval[row]
            //   newMax = max(oldMax, x)
            //   sumExp[row] = sumExp[row]*exp(oldMax - newMax) + exp(x - newMax)
            //   maxval[row] = newMax

            // We'll do it HPC style: each HPC warpgroup handles some subset of rows.
            // For example, row r in [warpgroupid, warpgroupid+CONSUMER_WARPGROUPS, ...].
            // We'll do a for-loop:
            for (int r = warpgroupid; r < ROWS; r += CONSUMER_WARPGROUPS) {
                // load partial-lse
                float x = partial_smem[stage](r, 0);

                // do the stable sum
                float oldMax = maxval[r];
                float newMax = (x > oldMax) ? x : oldMax;
                float scaleOld = expf(oldMax - newMax);
                float scaleNew = expf(x      - newMax);
                sumExp[r] = sumExp[r]*scaleOld + scaleNew;
                maxval[r] = newMax;
            }

            // HPC warpgroup sync so that we don't overwrite partial_smem if multiple warps read it
            warpgroup::sync();
            
            // Done reading chunk c => let the producer reuse partial_smem[stage]
            if (warpgroup::laneid() == 0) {
                arrive(compute_done[stage], 1);
            }
        }

        // Now we've combined all chunks. We have final:
        //   final_lse[r] = log(sumExp[r]) + maxval[r].
        // We'll store that in shared memory or directly TMA store.

        // We'll do a quick HPC approach if we want each HPC warpgroup to handle different row. 
        // Then we store to global memory. Let's gather them in shared memory first, or store direct.

        // Example: store direct from each HPC warpgroup’s lanes. Then we do a second HPC approach
        // if we must. For simplicity, we do a lane = 0 store.

        // Wait for all HPC warpgroups to finish the for-loops. (We do a block-level barrier.)
        // But in HPC style, we can do warpgroup::sync(WARPGROUP_BARRIER) for block-level. 
        // Or simpler, __syncthreads() if we can. HPC has a utility but let's do __syncthreads here.
        __syncthreads();

        // Now store final LSE. We'll do a loop over the rows we own again:
        for (int r = warpgroupid; r < ROWS; r += CONSUMER_WARPGROUPS) {
            // final = log(sumExp[r]) + maxval[r]
            float finalVal = (sumExp[r] <= 0.f)
                             ? base_types::constants<float>::neg_infty()
                             : logf(sumExp[r]) + maxval[r];

            // TMA store can be done from lane 0 or any lane. We'll pick lane 0:
            if (localwarp == 0) {
                // We'll place it in a st_fl<ROWS,1> tile in shared memory temporarily:
                partial_tile final_smem;
                final_smem(r, 0) = finalVal;
                
                // The rest of the rows get 0 or -∞. 
                // If we want a complete tile, we should fill them, or do bounding in TMA.
                // For simplicity, store only row r, fill out-of-range with -∞:
                for(int rr=0; rr<ROWS; rr++){
                    if(rr != r) final_smem(rr,0) = base_types::constants<float>::neg_infty();
                }

                // Then do TMA store_async
                coord<partial_tile> final_coord = {0, 0}; // e.g. if final_lse is [ROWS,1] shape, or if we want offset r
                // But we actually want to place row r in global. Usually we do {r,0} if shape is [ROWS,1].
                // So:
                final_coord[0] = r;
                tma::store_async(g.final_lse, final_smem, final_coord);
                tma::store_async_wait(); // ensure it completes
            }
        }
        // HPC barrier to ensure all done
        warpgroup::sync();
    }
}
